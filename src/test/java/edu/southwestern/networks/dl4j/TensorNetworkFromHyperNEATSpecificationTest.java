package edu.southwestern.networks.dl4j;

import static org.junit.Assert.assertArrayEquals;

import java.util.List;

import org.junit.Test;

import edu.southwestern.MMNEAT.MMNEAT;
import edu.southwestern.evolution.EvolutionaryHistory;
import edu.southwestern.evolution.genotypes.HyperNEATCPPNGenotype;
import edu.southwestern.evolution.genotypes.TWEANNGenotype;
import edu.southwestern.networks.ActivationFunctions;
import edu.southwestern.networks.TWEANN;
import edu.southwestern.networks.hyperneat.HyperNEATTask;
import edu.southwestern.networks.hyperneat.HyperNEATUtil;
import edu.southwestern.networks.hyperneat.Substrate;
import edu.southwestern.parameters.Parameters;
import edu.southwestern.tasks.rlglue.tetris.HyperNEATTetrisTask;
import edu.southwestern.util.random.RandomNumbers;

public class TensorNetworkFromHyperNEATSpecificationTest {

	@Test
	public void testFillWeightsFromHyperNEATNetworkAllReLU() {
		RandomNumbers.reset(5); // Just some fixed seed
		HyperNEATTetrisTask.hardSubstrateReset();
		EvolutionaryHistory.archetypes = null; // Force reset
		MMNEAT.clearClasses();
		Parameters.initializeParameterCollections(new String[] {"runNumber:0","randomSeed:0","io:false","netio:false","maxGens:10","watch:false",
				"task:edu.southwestern.tasks.rlglue.tetris.HyperNEATTetrisTask",
				"rlGlueEnvironment:org.rlcommunity.environments.tetris.Tetris",
				"rlGlueExtractor:edu.southwestern.tasks.rlglue.featureextractors.tetris.RawTetrisStateExtractor",
				"rlGlueAgent:edu.southwestern.tasks.rlglue.tetris.TetrisAfterStateAgent",
				"splitRawTetrisInputs:true",
				"senseHolesDifferently:true",
				"hyperNEAT:true", // Prevents extra bias input
				"inputsUseID:true", // DL4J does this, so the HyperNEAT network must too to be compatible
				"linkExpressionThreshold:-0.1", // Express all links
				"stride:1","receptiveFieldHeight:3","receptiveFieldWidth:3","zeroPadding:false","convolutionWeightSharing:true",
				"HNProcessDepth:4","HNProcessWidth:4","convolution:true",
				"ftype:"+ActivationFunctions.FTYPE_RE_LU, "heterogeneousSubstrateActivations:false",
				"experiment:edu.southwestern.experiment.rl.EvaluateDL4JNetworkExperiment"});
		MMNEAT.loadClasses(); // Load parameters specified above
		
		HyperNEATTask hnt = HyperNEATUtil.getHyperNEATTask();		
		TensorNetworkFromHyperNEATSpecification tensorNetwork = new TensorNetworkFromHyperNEATSpecification(hnt);
		
        HyperNEATCPPNGenotype cppn = new HyperNEATCPPNGenotype();
        // Network generated by CPPN
        TWEANNGenotype substrateGenotype = cppn.getSubstrateGenotype(hnt);
        // DL4J network weights replaced with weights from HyperNEAT network
        tensorNetwork.fillWeightsFromHyperNEATNetwork(hnt, substrateGenotype);   
        // Wrap using my Network interface
		List<Substrate> substrates = hnt.getSubstrateInformation();
		int[] inputShape = HyperNEATUtil.getInputShape(substrates);
		int outputCount = HyperNEATUtil.getOutputCount(substrates);
		// DL4JNetworkWrapper implements Network
		DL4JNetworkWrapper dl4jNetwork = new DL4JNetworkWrapper(tensorNetwork, inputShape, outputCount);
        TWEANN substrateNetwork = substrateGenotype.getPhenotype();

//        System.out.println("testFillWeightsFromHyperNEATNetworkAllReLU");
//        MiscUtil.waitForReadStringAndEnterKeyPress();
		
        // Test 10 random inputs
		for(int i = 0; i < 10; i++) {
			// Should not be recurrent, but flush just in case
			substrateNetwork.flush();
			dl4jNetwork.flush();
			
	        double[] randomInput = RandomNumbers.randomArray(substrateGenotype.numIn);
	        double[] hyperNEATOutput = substrateNetwork.process(randomInput);
	        double[] dl4jOutput = dl4jNetwork.process(randomInput);

        // Debugging output
//        System.out.println("Inputs: " + Arrays.toString(randomInput));
//        for(Node n : substrateNetwork.nodes) {
//        	System.out.println(n.innovation + ":" + n.ftype + ":" + n.getActivation());
//        }
//        for(Layer lay : tensorNetwork.model.getLayers()) {
//        	System.out.println(lay.input());
//        }
//        
//        System.out.println("hyperNEATOutput: " + Arrays.toString(hyperNEATOutput));
//        System.out.println("dl4jOutput: " + Arrays.toString(dl4jOutput));

	        // The error margin here is fairly large. This is (presumably) because
	        // ND4J (I believe) favors float instead of double.
        	assertArrayEquals(hyperNEATOutput, dl4jOutput, 0.2);
		}
		
		
		// UNFORTUNATE RESULT:
		// The commented test below fails, because apparently DL4J networks are NOT faster than
		// my TWEANN class, which kind of surprises me. It's possible that using a GPU backend
		// or a different machine would be faster, but currently the result below indicates that
		// using DL4J networks instead of TWEANNs does not make sense in general. However, DL4J
		// networks are still useful for situations where backprop is needed, which is part of
		// any RL with NNs. Further exploration is needed.
		
		
		// Now do timing benchmark
//		final int CYCLES = 100;
//	    double[] randomInput = RandomNumbers.randomArray(substrateGenotype.numIn);
//	    
//	    long startDL4J = System.currentTimeMillis();
//	    for(int i = 0; i < CYCLES; i++) {
//			// Should not be recurrent, but flush just in case
//			dl4jNetwork.flush();	
//	        dl4jNetwork.process(randomInput);
//		}
//	    long endDL4J = System.currentTimeMillis();
//	    
//	    long timeDL4J = endDL4J - startDL4J;
//	    
//	    long startHyperNEAT = System.currentTimeMillis();
//	    for(int i = 0; i < CYCLES; i++) {
//			// Should not be recurrent, but flush just in case
//			substrateNetwork.flush();
//		    substrateNetwork.process(randomInput);
//	    }
//	    long endHyperNEAT = System.currentTimeMillis();
//	    
//	    long timeHyperNEAT = endHyperNEAT - startHyperNEAT;
//	    
//	    System.out.println("timeDL4J:"+timeDL4J);
//	    System.out.println("timeHyperNEAT:"+timeHyperNEAT);
//	    
//	    MiscUtil.waitForReadStringAndEnterKeyPress();
//	    
//	    assertTrue(timeHyperNEAT > timeDL4J);
	}

	@Test
	public void testFillWeightsFromHyperNEATNetworkAllTanH() {
		HyperNEATTetrisTask.hardSubstrateReset();
		EvolutionaryHistory.archetypes = null; // Force reset
		MMNEAT.clearClasses();
		Parameters.initializeParameterCollections(new String[] {"runNumber:0","randomSeed:0","io:false","netio:false","maxGens:10","watch:false",
				"task:edu.southwestern.tasks.rlglue.tetris.HyperNEATTetrisTask",
				"rlGlueEnvironment:org.rlcommunity.environments.tetris.Tetris",
				"rlGlueExtractor:edu.southwestern.tasks.rlglue.featureextractors.tetris.RawTetrisStateExtractor",
				"rlGlueAgent:edu.southwestern.tasks.rlglue.tetris.TetrisAfterStateAgent",
				"splitRawTetrisInputs:true",
				"senseHolesDifferently:true",
				"hyperNEAT:true", // Prevents extra bias input
				"inputsUseID:true", // DL4J does this, so the HyperNEAT network must too to be compatible
				"linkExpressionThreshold:-0.1", // Express all links
				"stride:1","receptiveFieldHeight:3","receptiveFieldWidth:3","zeroPadding:false","convolutionWeightSharing:true",
				"HNProcessDepth:4","HNProcessWidth:4","convolution:true",
				// This is how this test differs from the previous
				"ftype:"+ActivationFunctions.FTYPE_TANH, "heterogeneousSubstrateActivations:false", 
				"experiment:edu.southwestern.experiment.rl.EvaluateDL4JNetworkExperiment"});
		MMNEAT.loadClasses(); // Load parameters specified above
		
		HyperNEATTask hnt = HyperNEATUtil.getHyperNEATTask();		
		TensorNetworkFromHyperNEATSpecification tensorNetwork = new TensorNetworkFromHyperNEATSpecification(hnt);
		
        HyperNEATCPPNGenotype cppn = new HyperNEATCPPNGenotype();
        // Network generated by CPPN
        TWEANNGenotype substrateGenotype = cppn.getSubstrateGenotype(hnt);
        // DL4J network weights replaced with weights from HyperNEAT network
        tensorNetwork.fillWeightsFromHyperNEATNetwork(hnt, substrateGenotype);   
        // Wrap using my Network interface
		List<Substrate> substrates = hnt.getSubstrateInformation();
		int[] inputShape = HyperNEATUtil.getInputShape(substrates);
		int outputCount = HyperNEATUtil.getOutputCount(substrates);
		// DL4JNetworkWrapper implements Network
		DL4JNetworkWrapper dl4jNetwork = new DL4JNetworkWrapper(tensorNetwork, inputShape, outputCount);
        TWEANN substrateNetwork = substrateGenotype.getPhenotype();
    
//        System.out.println("testFillWeightsFromHyperNEATNetworkAllTanH");
//        MiscUtil.waitForReadStringAndEnterKeyPress();
        
		// Test 10 random inputs
		for(int i = 0; i < 10; i++) {
			// Should not be recurrent, but flush just in case
			substrateNetwork.flush();
			dl4jNetwork.flush();
			
	        double[] randomInput = RandomNumbers.randomArray(substrateGenotype.numIn);
	        double[] hyperNEATOutput = substrateNetwork.process(randomInput);
	        double[] dl4jOutput = dl4jNetwork.process(randomInput);
	        
//	        System.out.println(Arrays.toString(hyperNEATOutput));
//	        System.out.println(Arrays.toString(dl4jOutput));
	        
	        // Precision here is smaller because the scale of tanh outputs is smaller
        	assertArrayEquals(hyperNEATOutput, dl4jOutput, 0.001);
		}
	}
	

	@Test
	public void testFillWeightsFromHyperNEATNetworkReLUWithTanHOnTop() {
		HyperNEATTetrisTask.hardSubstrateReset();
		EvolutionaryHistory.archetypes = null; // Force reset
		MMNEAT.clearClasses();
		Parameters.initializeParameterCollections(new String[] {"runNumber:0","randomSeed:0","io:false","netio:false","maxGens:10","watch:false",
				"task:edu.southwestern.tasks.rlglue.tetris.HyperNEATTetrisTask",
				"rlGlueEnvironment:org.rlcommunity.environments.tetris.Tetris",
				"rlGlueExtractor:edu.southwestern.tasks.rlglue.featureextractors.tetris.RawTetrisStateExtractor",
				"rlGlueAgent:edu.southwestern.tasks.rlglue.tetris.TetrisAfterStateAgent",
				"splitRawTetrisInputs:true",
				"senseHolesDifferently:true",
				"hyperNEAT:true", // Prevents extra bias input
				"inputsUseID:true", // DL4J does this, so the HyperNEAT network must too to be compatible
				"linkExpressionThreshold:-0.1", // Express all links
				"stride:1","receptiveFieldHeight:3","receptiveFieldWidth:3","zeroPadding:false","convolutionWeightSharing:true",
				"HNProcessDepth:4","HNProcessWidth:4","convolution:true",
				// This is how this test differs from the previous
				"ftype:"+ActivationFunctions.FTYPE_TANH, "heterogeneousSubstrateActivations:true", 
				"experiment:edu.southwestern.experiment.rl.EvaluateDL4JNetworkExperiment"});
		MMNEAT.loadClasses(); // Load parameters specified above
		
		HyperNEATTask hnt = HyperNEATUtil.getHyperNEATTask();		
		TensorNetworkFromHyperNEATSpecification tensorNetwork = new TensorNetworkFromHyperNEATSpecification(hnt);
		
        HyperNEATCPPNGenotype cppn = new HyperNEATCPPNGenotype();
        // Network generated by CPPN
        TWEANNGenotype substrateGenotype = cppn.getSubstrateGenotype(hnt);
        // DL4J network weights replaced with weights from HyperNEAT network
        tensorNetwork.fillWeightsFromHyperNEATNetwork(hnt, substrateGenotype);   
        // Wrap using my Network interface
		List<Substrate> substrates = hnt.getSubstrateInformation();
		int[] inputShape = HyperNEATUtil.getInputShape(substrates);
		int outputCount = HyperNEATUtil.getOutputCount(substrates);
		// DL4JNetworkWrapper implements Network
		DL4JNetworkWrapper dl4jNetwork = new DL4JNetworkWrapper(tensorNetwork, inputShape, outputCount);
        TWEANN substrateNetwork = substrateGenotype.getPhenotype();
    
//        System.out.println("testFillWeightsFromHyperNEATNetworkReLUWithTanHOnTop");
//        MiscUtil.waitForReadStringAndEnterKeyPress();        

        // Test 10 random inputs
		for(int i = 0; i < 10; i++) {
			// Should not be recurrent, but flush just in case
			substrateNetwork.flush();
			dl4jNetwork.flush();
			
	        double[] randomInput = RandomNumbers.randomArray(substrateGenotype.numIn);
	        double[] hyperNEATOutput = substrateNetwork.process(randomInput);
	        double[] dl4jOutput = dl4jNetwork.process(randomInput);
	        
//	        System.out.println(Arrays.toString(hyperNEATOutput));
//	        System.out.println(Arrays.toString(dl4jOutput));
	        
	        // Precision here is smaller because the scale of tanh outputs is smaller
        	assertArrayEquals(hyperNEATOutput, dl4jOutput, 0.001);
		}
	}
}
